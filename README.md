🧠 LLM Evaluation with Deepeval

A hands-on exploration of Deepeval for evaluating and red-teaming large language models (LLMs).
This repository documents my effort in this journey of testing, benchmarking, and validating LLM reliability using custom prompts, metrics, and pipelines.

🔍 What’s inside
* Experiments with Deepeval metrics (faithfulness, coherence, toxicity, factuality, etc.)
* Prompt test suites for structured and unstructured LLM outputs
* Integrations with Promptflow, Promptfoo, and CI/CD pipelines
* I wil try to work on real-world case studies and evaluation reports
* Continuous progress logs, learnings, and comparisons

🚀 Goal
To build a reusable evaluation framework for LLM performance testing and share best practices for AI quality assurance, bias detection, and red-teaming.